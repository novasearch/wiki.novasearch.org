{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "Download the \"Sentiment Polarity Dataset Version 2.0\" from http://www.nltk.org/nltk_data/ and put in the defined folder. \n",
    "The dataset zip file is here: https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/movie_reviews.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_files\n",
    "moviedir = r'./movie_reviews'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preparation\n",
    "### Load the dataset and inspect its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie = load_files(moviedir, shuffle=True)\n",
    "len(movie.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"arnold schwarzenegger has been an icon for action enthusiasts , since the late 80's , but lately his films have been very sloppy and the one-liners are getting worse . \\nit's hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what's it matter to him ? \\nonce again arnold has signed to do another expensive blockbuster , that can't compare with the likes of the terminator series , true lies and even eraser . \\nin this so called dark thriller , the devil ( gabriel byrne ) has come upon earth , to impregnate a woman ( robin tunney ) which happens every 1000 years , and basically destroy the world , but apparently god has chosen one man , and that one man is jericho cane ( arnold himself ) . \\nwith the help of a trusty sidekick ( kevin pollack ) , they will stop at nothing to let the devil take over the world ! \\nparts of this are actually so absurd , that they would fit right in with dogma . \\nyes , the film is that weak , but it's better than the other blockbuster right now ( sleepy hollow ) , but it makes the world is not enough look like a 4 star film . \\nanyway , this definitely doesn't seem like an arnold movie . \\nit just wasn't the type of film you can see him doing . \\nsure he gave us a few chuckles with his well known one-liners , but he seemed confused as to where his character and the film was going . \\nit's understandable , especially when the ending had to be changed according to some sources . \\naside form that , he still walked through it , much like he has in the past few films . \\ni'm sorry to say this arnold but maybe these are the end of your action days . \\nspeaking of action , where was it in this film ? \\nthere was hardly any explosions or fights . \\nthe devil made a few places explode , but arnold wasn't kicking some devil butt . \\nthe ending was changed to make it more spiritual , which undoubtedly ruined the film . \\ni was at least hoping for a cool ending if nothing else occurred , but once again i was let down . \\ni also don't know why the film took so long and cost so much . \\nthere was really no super affects at all , unless you consider an invisible devil , who was in it for 5 minutes tops , worth the overpriced budget . \\nthe budget should have gone into a better script , where at least audiences could be somewhat entertained instead of facing boredom . \\nit's pitiful to see how scripts like these get bought and made into a movie . \\ndo they even read these things anymore ? \\nit sure doesn't seem like it . \\nthankfully gabriel's performance gave some light to this poor film . \\nwhen he walks down the street searching for robin tunney , you can't help but feel that he looked like a devil . \\nthe guy is creepy looking anyway ! \\nwhen it's all over , you're just glad it's the end of the movie . \\ndon't bother to see this , if you're expecting a solid action flick , because it's neither solid nor does it have action . \\nit's just another movie that we are suckered in to seeing , due to a strategic marketing campaign . \\nsave your money and see the world is not enough for an entertaining experience . \\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie.target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data between train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jmag/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(movie.data, movie.target, \n",
    "                                                          test_size = 0.20, random_state = 12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute word dictionaries and word-doc frequencies matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 3000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieVzer= CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize, max_features=3000)\n",
    "docs_train_counts = movieVzer.fit_transform(docs_train)\n",
    "docs_test_counts = movieVzer.transform(docs_test)\n",
    "docs_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF weighting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieTfmer = TfidfTransformer()\n",
    "docs_train_tfidf = movieTfmer.fit_transform(docs_train_counts)\n",
    "docs_test_tfidf = movieTfmer.transform(docs_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    " https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    " \n",
    " -  Parameters relevant to class inbalance: class_weight.\n",
    " - Parameters relevant to regularization: penalty, C.\n",
    " - Paremeters relevant to stop criteria: tol, max_iter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(docs_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are given by the coef_ variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.98274511 -0.02516272 -0.07721108 ...  0.09443541  0.07750834\n",
      "   0.02408974]]\n"
     ]
    }
   ],
   "source": [
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "predict_train = clf.predict(docs_train_tfidf)\n",
    "predicted_test = clf.predict(docs_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.90      0.91      0.90       785\n",
      "         pos       0.91      0.90      0.91       815\n",
      "\n",
      "    accuracy                           0.91      1600\n",
      "   macro avg       0.91      0.91      0.91      1600\n",
      "weighted avg       0.91      0.91      0.91      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['neg', 'pos']\n",
    "print(classification_report(predict_train, y_train, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.73      0.81      0.77       186\n",
      "         pos       0.82      0.74      0.78       214\n",
      "\n",
      "    accuracy                           0.78       400\n",
      "   macro avg       0.78      0.78      0.77       400\n",
      "weighted avg       0.78      0.78      0.78       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predicted_test, y_test, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "The above implementation is done with scikit learn library which provides an series of off-the-shelf algorithms for machine learning. In this exercise you should use PyTorch to implement all the steps of the classifier. The word embeddings example in the previous lab already provides you a good example of how PyTorch works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
